{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'causes_death.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0f447c4c7a75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading deaths risks dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrisk_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'causes_death.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrisk_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'causes_death.csv'"
     ]
    }
   ],
   "source": [
    "# Loading deaths risks dataset\n",
    "\n",
    "risk_data = pd.read_csv('causes_death.csv')\n",
    "\n",
    "risk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of old column names to new simplified column names\n",
    "column_mapping = {\n",
    "    'Deaths - Invasive Non-typhoidal Salmonella (iNTS) - Sex: Both - Age: Under 5 (Number)': 'Invasive Non-typhoidal Salmonella (iNTS)',\n",
    "    'Deaths - Interpersonal violence - Sex: Both - Age: Under 5 (Number)': 'Interpersonal violence',\n",
    "    'Deaths - Nutritional deficiencies - Sex: Both - Age: Under 5 (Number)': 'Nutritional deficiencies',\n",
    "    'Deaths - Acute hepatitis - Sex: Both - Age: Under 5 (Number)': 'Acute hepatitis',\n",
    "    'Deaths - Neoplasms - Sex: Both - Age: Under 5 (Number)': 'Neoplasms',\n",
    "    'Deaths - Measles - Sex: Both - Age: Under 5 (Number)': 'Measles',\n",
    "    'Deaths - Digestive diseases - Sex: Both - Age: Under 5 (Number)': 'Digestive diseases',\n",
    "    'Deaths - Cirrhosis and other chronic liver diseases - Sex: Both - Age: Under 5 (Number)': 'Cirrhosis and other chronic liver diseases',\n",
    "    'Deaths - Chronic kidney disease - Sex: Both - Age: Under 5 (Number)': 'Chronic kidney disease',\n",
    "    'Deaths - Cardiovascular diseases - Sex: Both - Age: Under 5 (Number)': 'Cardiovascular diseases',\n",
    "    'Deaths - Congenital birth defects - Sex: Both - Age: Under 5 (Number)': 'Congenital birth defects',\n",
    "    'Deaths - Lower respiratory infections - Sex: Both - Age: Under 5 (Number)': 'Lower respiratory infections',\n",
    "    'Deaths - Neonatal preterm birth - Sex: Both - Age: Under 5 (Number)': 'Neonatal preterm birth',\n",
    "    'Deaths - Environmental heat and cold exposure - Sex: Both - Age: Under 5 (Number)': 'Environmental heat and cold exposure',\n",
    "    'Deaths - Neonatal sepsis and other neonatal infections - Sex: Both - Age: Under 5 (Number)': 'Neonatal sepsis and other neonatal infections',\n",
    "    'Deaths - Exposure to forces of nature - Sex: Both - Age: Under 5 (Number)': 'Exposure to forces of nature',\n",
    "    'Deaths - Diabetes mellitus - Sex: Both - Age: Under 5 (Number)': 'Diabetes mellitus',\n",
    "    'Deaths - Neonatal encephalopathy due to birth asphyxia and trauma - Sex: Both - Age: Under 5 (Number)': 'Neonatal encephalopathy due to birth asphyxia and trauma',\n",
    "    'Deaths - Meningitis - Sex: Both - Age: Under 5 (Number)': 'Meningitis',\n",
    "    'Deaths - Other neonatal disorders - Sex: Both - Age: Under 5 (Number)': 'Other neonatal disorders',\n",
    "    'Deaths - Whooping cough - Sex: Both - Age: Under 5 (Number)': 'Whooping cough',\n",
    "    'Deaths - Diarrheal diseases - Sex: Both - Age: Under 5 (Number)': 'Diarrheal diseases',\n",
    "    'Deaths - Fire, heat, and hot substances - Sex: Both - Age: Under 5 (Number)': 'Fire, heat, and hot substances',\n",
    "    'Deaths - Road injuries - Sex: Both - Age: Under 5 (Number)': 'Road injuries',\n",
    "    'Deaths - Tuberculosis - Sex: Both - Age: Under 5 (Number)': 'Tuberculosis',\n",
    "    'Deaths - HIV/AIDS - Sex: Both - Age: Under 5 (Number)': 'HIV/AIDS',\n",
    "    'Deaths - Drowning - Sex: Both - Age: Under 5 (Number)': 'Drowning',\n",
    "    'Deaths - Malaria - Sex: Both - Age: Under 5 (Number)': 'Malaria',\n",
    "    'Deaths - Syphilis - Sex: Both - Age: Under 5 (Number)': 'Syphilis'\n",
    "}\n",
    "\n",
    "# Rename columns using the defined mapping\n",
    "risk_data.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Display the DataFrame with simplified column names\n",
    "risk_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into a specific column Entity as we can see above we have non_countries such as world bank represented\n",
    "#how many other non_countries are there?\n",
    "column_name = risk_data['Entity']  \n",
    "\n",
    "# Display the unique values or any other analysis of the specific column\n",
    "print(column_name.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do away with the non_country entries because the do not focus on specific countries hence making them\n",
    "#irrelevant to the analysis.\n",
    "\n",
    "non_countries = [\n",
    "    'African Region (WHO)', 'American Samoa', 'East Asia & Pacific (WB)',\n",
    "    'Eastern Mediterranean Region (WHO)', 'England', 'Europe & Central Asia (WB)',\n",
    "    'European Region (WHO)', 'G20', 'Latin America & Caribbean (WB)',\n",
    "    'Micronesia (country)', 'Middle East & North Africa (WB)', 'North America (WB)',\n",
    "    'North Korea', 'Northern Ireland', 'Northern Mariana Islands', 'OECD Countries',\n",
    "    'Palestine', 'Region of the Americas (WHO)', 'Scotland', 'South Asia (WB)',\n",
    "    'South-East Asia Region (WHO)', 'Sub-Saharan Africa (WB)', 'Taiwan',\n",
    "    'Western Pacific Region (WHO)', 'World', 'World Bank High Income',\n",
    "    'World Bank Low Income', 'World Bank Lower Middle Income',\n",
    "    'World Bank Upper Middle Income', 'Wales'\n",
    "]\n",
    "\n",
    "# Filtering out non-country entities\n",
    "risk_data = risk_data[~risk_data['Entity'].isin(non_countries)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading deaths risks dataset\n",
    "\n",
    "child_data = pd.read_csv('per-capita-total-expenditure-on-health-vs-child-mortality.csv')\n",
    "\n",
    "child_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Year' column to numeric (in case it's not already)\n",
    "child_data['Year'] = pd.to_numeric(child_data['Year'], errors='coerce')\n",
    "\n",
    "# Filter the DataFrame to include only years between 1990 and 2020\n",
    "child_data_filtered = child_data[(child_data['Year'] >= 1990) & (child_data['Year'] <= 2020)]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "child_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns \n",
    "child_data_filtered.drop(columns=['Current health expenditure per capita, PPP (current international $)','Continent'], inplace=True)\n",
    "\n",
    "# Display the DataFrame after dropping columns\n",
    "child_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_data_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on the 'Entity'/'Country Name', 'Code', and 'Year' columns\n",
    "merged_df = pd.merge(risk_data, child_data_filtered, on=['Entity', 'Code', 'Year'])\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "sex_data = pd.read_csv('mortality_rates_sexes.csv', encoding='latin1')\n",
    "\n",
    "sex_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns \n",
    "sex_data.drop(columns=['Population (historical estimates)', 'Continent'], inplace=True)\n",
    "\n",
    "# Display the DataFrame after dropping columns\n",
    "sex_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on the 'Entity'/'Country Name', 'Code', and 'Year' columns\n",
    "merged_df2 = pd.merge(merged_df, sex_data, on=['Entity', 'Code', 'Year'])\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df2.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "age_data = pd.read_csv('deaths_ages.csv', encoding='latin1')\n",
    "\n",
    "age_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on the 'Entity'/'Country Name', 'Code', and 'Year' columns\n",
    "merged_df3 = pd.merge(merged_df2, age_data, on=['Entity', 'Code', 'Year'])\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df3 = merged_df3.drop(columns=['Code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values and NaN\n",
    "null_values = merged_df3.isnull().sum()\n",
    "\n",
    "# Display the count of null values for each column\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_with_null = ['Mortality rate, under-5 (per 1,000 live births)', 'Mortality rate, under-5, female (per 1,000 live births)', 'Mortality rate, under-5, male (per 1,000 live births)']\n",
    "\n",
    "# Filter the DataFrame to display rows where any of the specified columns have null values\n",
    "null_rows = merged_df3[merged_df3[columns_with_null].isnull().any(axis=1)]\n",
    "\n",
    "# Display the rows where any of the specified columns have null values\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df3 = merged_df3.dropna()\n",
    "\n",
    "merged_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the GDP dataset\n",
    "\n",
    "GDP_data = pd.read_csv('gdp.csv', encoding='latin1')\n",
    "\n",
    "GDP_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Melt the DataFrame to convert column data into rows\n",
    "melted_df = pd.melt(GDP_data, id_vars=['Country Name'], var_name='Year', value_name='GDP')\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(melted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values and NaN\n",
    "null_values = melted_df.isnull().sum()\n",
    "\n",
    "# Display the count of null values for each column\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df = melted_df.dropna()\n",
    "\n",
    "gdp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df['Year'] = gdp_df['Year'].astype(int)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(gdp_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on the 'Country Name' and 'Entity' columns\n",
    "final_df = pd.merge(merged_df3, gdp_df, left_on=['Entity','Year'], right_on=['Country Name','Year'])\n",
    "\n",
    "# Drop the redundant 'Entity' column\n",
    "final_df.drop('Country Name', axis=1, inplace=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values and NaN\n",
    "null_values = final_df.isnull().sum()\n",
    "\n",
    "# Display the count of null values for each column\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "# Handling Outliers\n",
    "# Initializing the IsolationForest model with a contamination parameter of 0.05\n",
    "model = IsolationForest(contamination=0.05, random_state=0)\n",
    "\n",
    "# Fitting the model on our dataset (converting DataFrame to NumPy to avoid warning)\n",
    "final_df['Outlier_Scores'] = model.fit_predict(final_df.iloc[:, 1:].to_numpy())\n",
    "\n",
    "# Creating a new column to identify outliers (1 for inliers and -1 for outliers)\n",
    "final_df['Is_Outlier'] = [1 if x == -1 else 0 for x in final_df['Outlier_Scores']]\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of inliers and outliers\n",
    "outlier_percentage = final_df['Is_Outlier'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plotting the percentage of inliers and outliers\n",
    "plt.figure(figsize=(12, 4))\n",
    "outlier_percentage.plot(kind='barh', color='#000080')\n",
    "\n",
    "# Adding the percentage labels on the bars\n",
    "for index, value in enumerate(outlier_percentage):\n",
    "    plt.text(value, index, f'{value:.2f}%', fontsize=15)\n",
    "\n",
    "plt.title('Percentage of Inliers and Outliers')\n",
    "plt.xticks(ticks=np.arange(0, 115, 5))\n",
    "plt.xlabel('Percentage (%)')\n",
    "plt.ylabel('Is Outlier')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the outliers for analysis\n",
    "outliers_data = final_df[final_df['Is_Outlier'] == 1]\n",
    "outliers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outliers from the main dataset\n",
    "mortality_df = final_df[final_df['Is_Outlier'] == 0]\n",
    "\n",
    "# Drop the 'Outlier_Scores' and 'Is_Outlier' columns\n",
    "mortality_df = mortality_df.drop(columns=['Outlier_Scores', 'Is_Outlier'])\n",
    "\n",
    "# Reset the index of the cleaned data\n",
    "mortality_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# Reset background style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Calculate the correlation matrix excluding the 'ID' column\n",
    "corr = mortality_df.drop(columns=['Entity']).corr()\n",
    "\n",
    "# Define a custom colormap\n",
    "colors = ['#0047AB', '#2D65C9', '#5993E5', '#B3D0F4', '#000080']\n",
    "my_cmap = LinearSegmentedColormap.from_list('custom_map', colors, N=256)\n",
    "\n",
    "# Create a mask to only show the lower triangle of the matrix (since it's mirrored around its \n",
    "# top-left to bottom-right diagonal)\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, mask=mask, cmap=my_cmap, annot=True, center=0, fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "corr_matrix = mortality_df.drop(columns=['Entity']).corr()\n",
    "\n",
    "# Print correlation matrix\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'Year' is the name of the column\n",
    "mortality_df['Year'] = mortality_df['Year'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# List of columns that don't need to be scaled\n",
    "columns_to_exclude = ['Entity', 'Year']\n",
    "\n",
    "# List of columns that need to be scaled\n",
    "columns_to_scale = mortality_df.columns.difference(columns_to_exclude)\n",
    "\n",
    "# Copy the cleaned dataset\n",
    "mortality_df_scaled = mortality_df.copy()\n",
    "\n",
    "# Applying the scaler to the necessary columns in the dataset\n",
    "mortality_df_scaled[columns_to_scale] = scaler.fit_transform(mortality_df_scaled[columns_to_scale])\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "mortality_df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction(PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "# Define the number of principal components to keep\n",
    "n_components = 6  \n",
    "mortality_df_scaled1 = mortality_df_scaled.drop(columns=['Entity', 'Year'])\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(mortality_df_scaled1)\n",
    "\n",
    "# Create a DataFrame for the principal components\n",
    "columns = [f'PC{i}' for i in range(1, n_components + 1)]\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=columns)\n",
    "\n",
    "# Add back the country column, if needed\n",
    "df_pca['Year'] = mortality_df_scaled['Year']\n",
    "\n",
    "# Display the DataFrame with principal components\n",
    "print(df_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set year as Index\n",
    "df_pca.set_index('Year', inplace=True)\n",
    "print(df_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#K-Means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "# Set plot style, and background color\n",
    "sns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})\n",
    "\n",
    "# Set the color palette for the plot\n",
    "sns.set_palette(['#ff6200'])\n",
    "\n",
    "# Instantiate the clustering model with the specified parameters\n",
    "km = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "\n",
    "# Create a figure and axis with the desired size\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the model and range of k values, and disable the timing plot\n",
    "visualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "visualizer.fit(df_pca)\n",
    "\n",
    "# Finalize and render the figure\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_analysis(df, start_k, stop_k, figsize=(15, 16)):\n",
    "    \"\"\"\n",
    "    Perform Silhouette analysis for a range of k values and visualize the results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Create a grid with (stop_k - start_k + 1) rows and 2 columns\n",
    "    grid = gridspec.GridSpec(stop_k - start_k + 1, 2)\n",
    "\n",
    "    # Assign the first plot to the first row and both columns\n",
    "    first_plot = plt.subplot(grid[0, :])\n",
    "\n",
    "    # First plot: Silhouette scores for different k values\n",
    "    sns.set_palette(['darkorange'])\n",
    "\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Iterate through the range of k values\n",
    "    for k in range(start_k, stop_k + 1):\n",
    "        km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "        km.fit(df)\n",
    "        labels = km.predict(df)\n",
    "        score = silhouette_score(df, labels)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "    best_k = start_k + silhouette_scores.index(max(silhouette_scores))\n",
    "\n",
    "    plt.plot(range(start_k, stop_k + 1), silhouette_scores, marker='o')\n",
    "    plt.xticks(range(start_k, stop_k + 1))\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('Average Silhouette Score for Different k Values', fontsize=15)\n",
    "\n",
    "    # Add the optimal k value text to the plot\n",
    "    optimal_k_text = f'The k value with the highest Silhouette score is: {best_k}'\n",
    "    plt.text(10, 0.23, optimal_k_text, fontsize=12, verticalalignment='bottom', \n",
    "             horizontalalignment='left', bbox=dict(facecolor='#fcc36d', edgecolor='#ff6200', boxstyle='round, pad=0.5'))\n",
    "             \n",
    "\n",
    "    # Second plot (subplot): Silhouette plots for each k value\n",
    "    colors = sns.color_palette(\"bright\")\n",
    "\n",
    "    for i in range(start_k, stop_k + 1):    \n",
    "        km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "        row_idx, col_idx = divmod(i - start_k, 2)\n",
    "\n",
    "        # Assign the plots to the second, third, and fourth rows\n",
    "        ax = plt.subplot(grid[row_idx + 1, col_idx])\n",
    "\n",
    "        visualizer = SilhouetteVisualizer(km, colors=colors, ax=ax)\n",
    "        visualizer.fit(df)\n",
    "\n",
    "        # Add the Silhouette score text to the plot\n",
    "        score = silhouette_score(df, km.labels_)\n",
    "        ax.text(0.97, 0.02, f'Silhouette Score: {score:.2f}', fontsize=12, \\\n",
    "                ha='right', transform=ax.transAxes, color='red')\n",
    "\n",
    "        ax.set_title(f'Silhouette Plot for {i} Clusters', fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "silhouette_analysis(df_pca, 3, 12, figsize=(20, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#choosing ( k = 3 ) is the better option\n",
    "# Apply KMeans clustering using the optimal k\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "kmeans.fit(df_pca)\n",
    "\n",
    "# Get the frequency of each cluster\n",
    "cluster_frequencies = Counter(kmeans.labels_)\n",
    "\n",
    "# Create a mapping from old labels to new labels based on frequency\n",
    "label_mapping = {label: new_label for new_label, (label, _) in \n",
    "                 enumerate(cluster_frequencies.most_common())}\n",
    "\n",
    "# Reverse the mapping to assign labels as per your criteria\n",
    "label_mapping = {v: k for k, v in {2: 1, 1: 0, 0: 2}.items()}\n",
    "\n",
    "# Apply the mapping to get the new labels\n",
    "new_labels = np.array([label_mapping[label] for label in kmeans.labels_])\n",
    "\n",
    "# Append the new cluster labels back to the original dataset\n",
    "mortality_df['cluster'] = new_labels\n",
    "\n",
    "# Append the new cluster labels to the PCA version of the dataset\n",
    "df_pca['cluster'] = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of countries in each cluster\n",
    "cluster_percentage = (df_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
    "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
    "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n",
    "\n",
    "# Adding percentages on the bars\n",
    "for index, value in enumerate(cluster_percentage['Percentage']):\n",
    "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
    "\n",
    "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
    "plt.xticks(ticks=np.arange(0, 50, 5))\n",
    "plt.xlabel('Percentage (%)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mortality_df['Year'] = pd.to_numeric(mortality_df['Year'], errors='coerce')\n",
    "\n",
    "# Excluding the 'Country Name' column from the calculation\n",
    "cluster_averages = mortality_df.drop(columns=['Entity']).groupby('cluster').mean()\n",
    "\n",
    "# Define a color palette\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(cluster_averages)))\n",
    "\n",
    "# Plotting separate bar graphs for each column with different colors for clusters\n",
    "for i, column in enumerate(cluster_averages.columns):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(cluster_averages.index, cluster_averages[column], color=colors)\n",
    "    plt.title(f'Average {column} by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(f'Average {column}')\n",
    "    plt.xticks(cluster_averages.index)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'Mortality Rate' column\n",
    "mortality_df_sorted = mortality_df.sort_values(by='Mortality rate, under-5 (per 1,000 live births)')\n",
    "\n",
    "# Group the sorted DataFrame by 'cluster'\n",
    "grouped = mortality_df_sorted.groupby('cluster')\n",
    "\n",
    "# List the top 5 countries in each cluster\n",
    "for cluster, group in grouped:\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    top_5_countries = group.head(5)\n",
    "    print(top_5_countries)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'Risk' column based on the 'cluster' column\n",
    "mortality_df['Risk'] = np.where(mortality_df['cluster'] == 0, 'High', 'Low')\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(mortality_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "X = mortality_df.drop(['Risk', 'Entity','cluster'], axis=1)  # Features (excluding 'Risk' and 'Entity' columns)\n",
    "y = mortality_df['Risk']  # Target variable ('Risk')\n",
    "\n",
    "# Step 2: Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Model Training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Other evaluation metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Get the coefficients of the logistic regression model\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame to display the feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort the DataFrame by absolute coefficient values to identify the most important features\n",
    "feature_importance_df['Absolute Coefficient'] = abs(feature_importance_df['Coefficient'])\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
